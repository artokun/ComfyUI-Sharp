{
  "timestamp": "2026-02-16T15:53:05.298863+00:00",
  "platform": "windows_portable",
  "hardware": {
    "os": "Windows-11-10.0.26100-SP0",
    "cpu": "Intel64 Family 6 Model 58 Stepping 9, GenuineIntel",
    "gpu": "NVIDIA RTX A4000"
  },
  "summary": {
    "total": 3,
    "passed": 0,
    "failed": 3
  },
  "workflows": [
    {
      "name": "pano",
      "status": "fail",
      "duration_seconds": 160.87,
      "error": "Workflow execution failed: {'prompt_id': '99561b5c-b4e6-4d6d-9b6c-e196af8343a8', 'node_id': '11', 'node_type': 'SharpPredictDepth', 'executed': ['9', '8', '6', '7'], 'exception_message': 'Input type (float) and bias type (struct c10::BFloat16) should be the same\\n', 'exception_type': 'RuntimeError', 'traceback': ['  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\execution.py\", line 527, in execute\\n    output_data, output_ui, has_subgraph, has_pending_tasks = await get_output_data(prompt_id, unique_id, obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb, v3_data=v3_data)\\n                                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\execution.py\", line 331, in get_output_data\\n    return_values = await _async_map_node_over_list(prompt_id, unique_id, obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb, v3_data=v3_data)\\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\execution.py\", line 305, in _async_map_node_over_list\\n    await process_inputs(input_dict, i)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\execution.py\", line 293, in process_inputs\\n    result = f(**inputs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\utils\\\\_contextlib.py\", line 124, in decorate_context\\n    return func(*args, **kwargs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\custom_nodes\\\\ComfyUI-Sharp\\\\nodes\\\\predict_depth.py\", line 157, in predict_depth\\n    monodepth_output, depth_decoder_features = predictor.encode(image_resized_pt)\\n                                               ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\custom_nodes\\\\ComfyUI-Sharp\\\\nodes\\\\sharp\\\\models\\\\predictor.py\", line 112, in encode\\n    monodepth_output = self.monodepth_model(image)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1776, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1787, in _call_impl\\n    return forward_call(*args, **kwargs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\custom_nodes\\\\ComfyUI-Sharp\\\\nodes\\\\sharp\\\\models\\\\monodepth.py\", line 197, in forward\\n    encoder_output = self.monodepth_predictor.encoder(inputs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1776, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1787, in _call_impl\\n    return forward_call(*args, **kwargs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\custom_nodes\\\\ComfyUI-Sharp\\\\nodes\\\\sharp\\\\models\\\\encoders\\\\spn_encoder.py\", line 249, in forward\\n    x_pyramid_encodings, patch_intermediate_features = self.patch_encoder(x_pyramid_patches)\\n                                                       ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1776, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1787, in _call_impl\\n    return forward_call(*args, **kwargs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\custom_nodes\\\\ComfyUI-Sharp\\\\nodes\\\\sharp\\\\models\\\\encoders\\\\vit_encoder.py\", line 70, in forward\\n    x = self.patch_embed(input_tensor)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1776, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1787, in _call_impl\\n    return forward_call(*args, **kwargs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\timm\\\\layers\\\\patch_embed.py\", line 136, in forward\\n    x = self.proj(x)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1776, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1787, in _call_impl\\n    return forward_call(*args, **kwargs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\conv.py\", line 553, in forward\\n    return self._conv_forward(input, self.weight, self.bias)\\n           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\conv.py\", line 548, in _conv_forward\\n    return F.conv2d(\\n           ~~~~~~~~^\\n        input, weight, bias, self.stride, self.padding, self.dilation, self.groups\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n'], 'current_inputs': {'model': [\"{'model_path': 'F:\\\\\\\\ComfyUI_windows_portable\\\\\\\\ComfyUI\\\\\\\\models\\\\\\\\sharp\\\\\\\\sharp_2572gikvuh.pt', 'dtype': torch.bfloat16}\"], 'image': ['tensor([[[[0.1753, 0.1119, 0.1057],\\n          [0.1854, 0.1170, 0.1077],\\n          [0.1868, 0.1151, 0.1028],\\n          ...,\\n          [0.1098, 0.0749, 0.0639],\\n          [0.1088, 0.0708, 0.0617],\\n          [0.1095, 0.0718, 0.0630]],\\n\\n         [[0.1713, 0.1091, 0.1027],\\n          [0.1894, 0.1201, 0.1097],\\n          [0.1887, 0.1179, 0.1045],\\n          ...,\\n          [0.0992, 0.0622, 0.0539],\\n          [0.0983, 0.0591, 0.0525],\\n          [0.1007, 0.0635, 0.0555]],\\n\\n         [[0.1661, 0.1060, 0.1002],\\n          [0.1864, 0.1179, 0.1070],\\n          [0.1931, 0.1213, 0.1065],\\n          ...,\\n          [0.0919, 0.0533, 0.0471],\\n          [0.0905, 0.0517, 0.0459],\\n          [0.0913, 0.0534, 0.0461]],\\n\\n         ...,\\n\\n         [[0.2313, 0.1777, 0.1414],\\n          [0.2374, 0.1830, 0.1450],\\n          [0.2445, 0.1907, 0.1515],\\n          ...,\\n          [0.1972, 0.1422, 0.1108],\\n          [0.1985, 0.1428, 0.1119],\\n          [0.2045, 0.1465, 0.1167]],\\n\\n         [[0.2353, 0.1841, 0.1503],\\n          [0.2410, 0.1888, 0.1541],\\n          [0.2466, 0.1947, 0.1584],\\n          ...,\\n          [0.2000, 0.1451, 0.1137],\\n          [0.2009, 0.1460, 0.1146],\\n          [0.2037, 0.1483, 0.1172]],\\n\\n         [[0.2414, 0.1904, 0.1601],\\n          [0.2429, 0.1916, 0.1590],\\n          [0.2454, 0.1944, 0.1599],\\n          ...,\\n          [0.2010, 0.1462, 0.1147],\\n          [0.2035, 0.1489, 0.1173],\\n          [0.2060, 0.1511, 0.1198]]],\\n\\n\\n        [[[0.1938, 0.1646, 0.1436],\\n          [0.1929, 0.1637, 0.1396],\\n          [0.1946, 0.1622, 0.1367],\\n          ...,\\n          [0.1624, 0.1248, 0.1042],\\n          [0.1899, 0.1459, 0.1208],\\n          [0.2193, 0.1689, 0.1390]],\\n\\n         [[0.1929, 0.1639, 0.1467],\\n          [0.1935, 0.1659, 0.1449],\\n          [0.1945, 0.1643, 0.1415],\\n          ...,\\n          [0.1695, 0.1306, 0.1087],\\n          [0.1944, 0.1488, 0.1227],\\n          [0.2214, 0.1706, 0.1394]],\\n\\n         [[0.1884, 0.1612, 0.1467],\\n          [0.1930, 0.1656, 0.1485],\\n          [0.1950, 0.1664, 0.1471],\\n          ...,\\n          [0.1730, 0.1328, 0.1101],\\n          [0.1960, 0.1498, 0.1226],\\n          [0.2207, 0.1706, 0.1377]],\\n\\n         ...,\\n\\n         [[0.2149, 0.1339, 0.1143],\\n          [0.2263, 0.1384, 0.1191],\\n          [0.2324, 0.1389, 0.1204],\\n          ...,\\n          [0.1607, 0.1215, 0.0918],\\n          [0.1601, 0.1212, 0.0938],\\n          [0.1586, 0.1205, 0.0935]],\\n\\n         [[0.2093, 0.1301, 0.1120],\\n          [0.2186, 0.1312, 0.1143],\\n          [0.2259, 0.1322, 0.1157],\\n          ...,\\n          [0.1597, 0.1216, 0.0920],\\n          [0.1604, 0.1216, 0.0937],\\n          [0.1618, 0.1221, 0.0953]],\\n\\n         [[0.2048, 0.1255, 0.1099],\\n          [0.2143, 0.1271, 0.1116],\\n          [0.2233, 0.1286, 0.1130],\\n          ...,\\n          [0.1579, 0.1218, 0.0946],\\n          [0.1607, 0.1228, 0.0969],\\n          [0.1640, 0.1237, 0.0995]]],\\n\\n\\n        [[[0.2273, 0.1709, 0.1381],\\n          [0.2485, 0.1858, 0.1513],\\n          [0.2934, 0.2230, 0.1899],\\n          ...,\\n          [0.4487, 0.2698, 0.1973],\\n          [0.5168, 0.3237, 0.2418],\\n          [0.5523, 0.3491, 0.2586]],\\n\\n         [[0.2349, 0.1820, 0.1522],\\n          [0.2400, 0.1822, 0.1481],\\n          [0.2806, 0.2143, 0.1804],\\n          ...,\\n          [0.4830, 0.3001, 0.2205],\\n          [0.5510, 0.3551, 0.2653],\\n          [0.5571, 0.3540, 0.2570]],\\n\\n         [[0.2382, 0.1888, 0.1614],\\n          [0.2475, 0.1933, 0.1622],\\n          [0.2718, 0.2103, 0.1767],\\n          ...,\\n          [0.5174, 0.3322, 0.2443],\\n          [0.5579, 0.3624, 0.2657],\\n          [0.5600, 0.3562, 0.2536]],\\n\\n         ...,\\n\\n         [[0.1884, 0.1437, 0.1064],\\n          [0.1774, 0.1338, 0.0981],\\n          [0.1760, 0.1305, 0.0951],\\n          ...,\\n          [0.2402, 0.1853, 0.1433],\\n          [0.2392, 0.1843, 0.1423],\\n          [0.2370, 0.1821, 0.1423]],\\n\\n         [[0.1863, 0.1429, 0.1066],\\n          [0.1831, 0.1388, 0.1038],\\n          [0.1836, 0.1362, 0.1020],\\n          ...,\\n          [0.2474, 0.1925, 0.1544],\\n          [0.2455, 0.1905, 0.1526],\\n          [0.2432, 0.1883, 0.1513]],\\n\\n         [[0.1903, 0.1471, 0.1108],\\n          [0.1885, 0.1426, 0.1083],\\n          [0.1881, 0.1389, 0.1066],\\n          ...,\\n          [0.2563, 0.2015, 0.1661],\\n          [0.2521, 0.1975, 0.1619],\\n          [0.2493, 0.1955, 0.1591]]],\\n\\n\\n        ...,\\n\\n\\n        [[[0.1448, 0.1053, 0.0920],\\n          [0.1473, 0.1068, 0.0930],\\n          [0.1441, 0.1002, 0.0871],\\n          ...,\\n          [0.2174, 0.1594, 0.1312],\\n          [0.2187, 0.1578, 0.1329],\\n          [0.2192, 0.1577, 0.1342]],\\n\\n         [[0.1384, 0.0991, 0.0836],\\n          [0.1466, 0.1066, 0.0913],\\n          [0.1475, 0.1042, 0.0903],\\n          ...,\\n          [0.2182, 0.1590, 0.1335],\\n          [0.2195, 0.1580, 0.1345],\\n          [0.2197, 0.1590, 0.1355]],\\n\\n         [[0.1336, 0.0925, 0.0777],\\n          [0.1414, 0.0993, 0.0851],\\n          [0.1484, 0.1053, 0.0916],\\n          ...,\\n          [0.2195, 0.1607, 0.1371],\\n          [0.2183, 0.1596, 0.1360],\\n          [0.2180, 0.1597, 0.1362]],\\n\\n         ...,\\n\\n         [[0.1700, 0.1060, 0.0804],\\n          [0.1736, 0.1116, 0.0800],\\n          [0.1577, 0.0996, 0.0627],\\n          ...,\\n          [0.0861, 0.0392, 0.0254],\\n          [0.0872, 0.0371, 0.0246],\\n          [0.0901, 0.0384, 0.0252]],\\n\\n         [[0.1588, 0.0956, 0.0687],\\n          [0.1466, 0.0903, 0.0542],\\n          [0.1379, 0.0847, 0.0429],\\n          ...,\\n          [0.0879, 0.0448, 0.0288],\\n          [0.0893, 0.0431, 0.0285],\\n          [0.0899, 0.0405, 0.0264]],\\n\\n         [[0.1345, 0.0769, 0.0465],\\n          [0.1327, 0.0782, 0.0407],\\n          [0.1360, 0.0801, 0.0418],\\n          ...,\\n          [0.0852, 0.0468, 0.0287],\\n          [0.0890, 0.0458, 0.0295],\\n          [0.0903, 0.0439, 0.0285]]],\\n\\n\\n        [[[0.1245, 0.0786, 0.0634],\\n          [0.1331, 0.0841, 0.0663],\\n          [0.1403, 0.0859, 0.0632],\\n          ...,\\n          [0.0941, 0.0858, 0.0692],\\n          [0.0941, 0.0813, 0.0676],\\n          [0.0981, 0.0839, 0.0704]],\\n\\n         [[0.1244, 0.0774, 0.0618],\\n          [0.1349, 0.0858, 0.0693],\\n          [0.1428, 0.0889, 0.0686],\\n          ...,\\n          [0.0932, 0.0813, 0.0675],\\n          [0.0964, 0.0815, 0.0692],\\n          [0.1047, 0.0911, 0.0772]],\\n\\n         [[0.1345, 0.0803, 0.0636],\\n          [0.1436, 0.0874, 0.0695],\\n          [0.1489, 0.0919, 0.0724],\\n          ...,\\n          [0.0932, 0.0794, 0.0676],\\n          [0.1036, 0.0898, 0.0769],\\n          [0.1127, 0.0998, 0.0854]],\\n\\n         ...,\\n\\n         [[0.2349, 0.1829, 0.1613],\\n          [0.2381, 0.1869, 0.1604],\\n          [0.2210, 0.1700, 0.1460],\\n          ...,\\n          [0.2127, 0.1629, 0.1384],\\n          [0.2208, 0.1710, 0.1510],\\n          [0.2084, 0.1586, 0.1391]],\\n\\n         [[0.2281, 0.1762, 0.1545],\\n          [0.2230, 0.1722, 0.1461],\\n          [0.2085, 0.1584, 0.1322],\\n          ...,\\n          [0.1889, 0.1412, 0.1132],\\n          [0.1926, 0.1462, 0.1223],\\n          [0.1874, 0.1428, 0.1242]],\\n\\n         [[0.2237, 0.1740, 0.1500],\\n          [0.2164, 0.1683, 0.1415],\\n          [0.2070, 0.1608, 0.1334],\\n          ...,\\n          [0.2059, 0.1564, 0.1241],\\n          [0.1871, 0.1417, 0.1137],\\n          [0.1735, 0.1323, 0.1089]]],\\n\\n\\n        [[[0.1115, 0.0645, 0.0427],\\n          [0.1110, 0.0647, 0.0439],\\n          [0.1122, 0.0676, 0.0513],\\n          ...,\\n          [0.0971, 0.0528, 0.0473],\\n          [0.1063, 0.0604, 0.0528],\\n          [0.1093, 0.0628, 0.0535]],\\n\\n         [[0.1120, 0.0649, 0.0436],\\n          [0.1094, 0.0631, 0.0408],\\n          [0.1099, 0.0653, 0.0459],\\n          ...,\\n          [0.1135, 0.0670, 0.0600],\\n          [0.1185, 0.0715, 0.0630],\\n          [0.1131, 0.0681, 0.0582]],\\n\\n         [[0.1089, 0.0637, 0.0447],\\n          [0.1068, 0.0626, 0.0430],\\n          [0.1059, 0.0627, 0.0431],\\n          ...,\\n          [0.1268, 0.0779, 0.0697],\\n          [0.1193, 0.0719, 0.0610],\\n          [0.1131, 0.0677, 0.0548]],\\n\\n         ...,\\n\\n         [[0.2512, 0.2059, 0.1667],\\n          [0.2512, 0.2040, 0.1652],\\n          [0.2527, 0.2032, 0.1664],\\n          ...,\\n          [0.1245, 0.0882, 0.0799],\\n          [0.1215, 0.0895, 0.0754],\\n          [0.1210, 0.0906, 0.0778]],\\n\\n         [[0.2528, 0.2060, 0.1673],\\n          [0.2516, 0.2022, 0.1652],\\n          [0.2516, 0.2006, 0.1651],\\n          ...,\\n          [0.1275, 0.0874, 0.0843],\\n          [0.1224, 0.0874, 0.0793],\\n          [0.1186, 0.0879, 0.0761]],\\n\\n         [[0.2532, 0.2033, 0.1655],\\n          [0.2520, 0.2008, 0.1643],\\n          [0.2533, 0.2024, 0.1647],\\n          ...,\\n          [0.1327, 0.0935, 0.0896],\\n          [0.1256, 0.0894, 0.0842],\\n          [0.1186, 0.0866, 0.0791]]]])'], 'extrinsics': ['tensor([[[-0.8090,  0.0000,  0.5878,  0.0000],\\n         [ 0.3699,  0.7771,  0.5091,  0.0000],\\n         [-0.4568,  0.6293, -0.6287,  0.0000],\\n         [ 0.0000,  0.0000,  0.0000,  1.0000]],\\n\\n        [[ 0.3090,  0.0000,  0.9511,  0.0000],\\n         [ 0.5985,  0.7771, -0.1945,  0.0000],\\n         [-0.7391,  0.6293,  0.2402,  0.0000],\\n         [ 0.0000,  0.0000,  0.0000,  1.0000]],\\n\\n        [[ 1.0000,  0.0000,  0.0000,  0.0000],\\n         [ 0.0000,  0.7771, -0.6293,  0.0000],\\n         [ 0.0000,  0.6293,  0.7771,  0.0000],\\n         [ 0.0000,  0.0000,  0.0000,  1.0000]],\\n\\n        [[ 0.3090,  0.0000, -0.9511,  0.0000],\\n         [-0.5985,  0.7771, -0.1945,  0.0000],\\n         [ 0.7391,  0.6293,  0.2402,  0.0000],\\n         [ 0.0000,  0.0000,  0.0000,  1.0000]],\\n\\n        [[-0.8090,  0.0000, -0.5878,  0.0000],\\n         [-0.3699,  0.7771,  0.5091,  0.0000],\\n         [ 0.4568,  0.6293, -0.6287,  0.0000],\\n         [ 0.0000,  0.0000,  0.0000,  1.0000]],\\n\\n        [[-0.8090,  0.0000,  0.5878,  0.0000],\\n         [-0.3201,  0.8387, -0.4406,  0.0000],\\n         [-0.4930, -0.5446, -0.6785,  0.0000],\\n         [ 0.0000,  0.0000,  0.0000,  1.0000]],\\n\\n        [[ 0.3090,  0.0000,  0.9511,  0.0000],\\n         [-0.5180,  0.8387,  0.1683,  0.0000],\\n         [-0.7976, -0.5446,  0.2592,  0.0000],\\n         [ 0.0000,  0.0000,  0.0000,  1.0000]],\\n\\n        [[ 1.0000,  0.0000,  0.0000,  0.0000],\\n         [ 0.0000,  0.8387,  0.5446,  0.0000],\\n         [ 0.0000, -0.5446,  0.8387,  0.0000],\\n         [ 0.0000,  0.0000,  0.0000,  1.0000]],\\n\\n        [[ 0.3090,  0.0000, -0.9511,  0.0000],\\n         [ 0.5180,  0.8387,  0.1683,  0.0000],\\n         [ 0.7976, -0.5446,  0.2592,  0.0000],\\n         [ 0.0000,  0.0000,  0.0000,  1.0000]],\\n\\n        [[-0.8090,  0.0000, -0.5878,  0.0000],\\n         [ 0.3201,  0.8387, -0.4406,  0.0000],\\n         [ 0.4930, -0.5446, -0.6785,  0.0000],\\n         [ 0.0000,  0.0000,  0.0000,  1.0000]],\\n\\n        [[-0.8090,  0.0000,  0.5878,  0.0000],\\n         [-0.5678, -0.2588, -0.7815,  0.0000],\\n         [ 0.1521, -0.9659,  0.2094,  0.0000],\\n         [ 0.0000,  0.0000,  0.0000,  1.0000]],\\n\\n        [[ 0.3090,  0.0000,  0.9511,  0.0000],\\n         [-0.9187, -0.2588,  0.2985,  0.0000],\\n         [ 0.2462, -0.9659, -0.0800,  0.0000],\\n         [ 0.0000,  0.0000,  0.0000,  1.0000]],\\n\\n        [[ 1.0000,  0.0000,  0.0000,  0.0000],\\n         [ 0.0000, -0.2588,  0.9659,  0.0000],\\n         [ 0.0000, -0.9659, -0.2588,  0.0000],\\n         [ 0.0000,  0.0000,  0.0000,  1.0000]],\\n\\n        [[ 0.3090,  0.0000, -0.9511,  0.0000],\\n         [ 0.9187, -0.2588,  0.2985,  0.0000],\\n         [-0.2462, -0.9659, -0.0800,  0.0000],\\n         [ 0.0000,  0.0000,  0.0000,  1.0000]],\\n\\n        [[-0.8090,  0.0000, -0.5878,  0.0000],\\n         [ 0.5678, -0.2588, -0.7815,  0.0000],\\n         [-0.1521, -0.9659,  0.2094,  0.0000],\\n         [ 0.0000,  0.0000,  0.0000,  1.0000]]])'], 'intrinsics': ['tensor([[768.0000,   0.0000, 767.5000,   0.0000],\\n        [  0.0000, 768.0000, 767.5000,   0.0000],\\n        [  0.0000,   0.0000,   1.0000,   0.0000],\\n        [  0.0000,   0.0000,   0.0000,   1.0000]])']}, 'current_outputs': ['9', '13', '20', '11', '24', '21', '25', '6', '16', '7', '12', '8', '19'], 'timestamp': 1771257079206}\n\nDetails:\nWorkflow: F:\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-Sharp\\workflows\\pano.json\nNode error: SharpPredictDepth",
      "hardware": {
        "os": "Windows-11-10.0.26100-SP0",
        "cpu": "Intel64 Family 6 Model 58 Stepping 9, GenuineIntel",
        "gpu": "NVIDIA RTX A4000"
      },
      "resources": {
        "ram": {
          "peak": 26.23,
          "avg": 21.89
        },
        "total_ram_gb": 30.0,
        "samples": 120,
        "vram": {
          "peak": 2.72,
          "avg": 1.2
        }
      }
    },
    {
      "name": "sharp_basic",
      "status": "fail",
      "duration_seconds": 49.36,
      "error": "Workflow execution failed: {'prompt_id': '1ed73d33-8506-4e3d-af19-87b4d9ecff79', 'node_id': '5', 'node_type': 'SharpPredict', 'executed': ['2', '1'], 'exception_message': 'Input type (float) and bias type (struct c10::BFloat16) should be the same\\n', 'exception_type': 'RuntimeError', 'traceback': ['  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\execution.py\", line 527, in execute\\n    output_data, output_ui, has_subgraph, has_pending_tasks = await get_output_data(prompt_id, unique_id, obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb, v3_data=v3_data)\\n                                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\execution.py\", line 331, in get_output_data\\n    return_values = await _async_map_node_over_list(prompt_id, unique_id, obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb, v3_data=v3_data)\\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\execution.py\", line 305, in _async_map_node_over_list\\n    await process_inputs(input_dict, i)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\execution.py\", line 293, in process_inputs\\n    result = f(**inputs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\utils\\\\_contextlib.py\", line 124, in decorate_context\\n    return func(*args, **kwargs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\custom_nodes\\\\ComfyUI-Sharp\\\\nodes\\\\predict.py\", line 190, in predict\\n    gaussians = self._predict_image_cached(\\n        predictor, image_np, f_px, device,\\n        extrinsics=img_extrinsics,\\n        intrinsics=img_intrinsics,\\n    )\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\custom_nodes\\\\ComfyUI-Sharp\\\\nodes\\\\predict.py\", line 284, in _predict_image_cached\\n    monodepth_output, _ = predictor.encode(image_resized_pt)\\n                          ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\custom_nodes\\\\ComfyUI-Sharp\\\\nodes\\\\sharp\\\\models\\\\predictor.py\", line 112, in encode\\n    monodepth_output = self.monodepth_model(image)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1776, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1787, in _call_impl\\n    return forward_call(*args, **kwargs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\custom_nodes\\\\ComfyUI-Sharp\\\\nodes\\\\sharp\\\\models\\\\monodepth.py\", line 197, in forward\\n    encoder_output = self.monodepth_predictor.encoder(inputs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1776, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1787, in _call_impl\\n    return forward_call(*args, **kwargs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\custom_nodes\\\\ComfyUI-Sharp\\\\nodes\\\\sharp\\\\models\\\\encoders\\\\spn_encoder.py\", line 249, in forward\\n    x_pyramid_encodings, patch_intermediate_features = self.patch_encoder(x_pyramid_patches)\\n                                                       ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1776, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1787, in _call_impl\\n    return forward_call(*args, **kwargs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\custom_nodes\\\\ComfyUI-Sharp\\\\nodes\\\\sharp\\\\models\\\\encoders\\\\vit_encoder.py\", line 70, in forward\\n    x = self.patch_embed(input_tensor)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1776, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1787, in _call_impl\\n    return forward_call(*args, **kwargs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\timm\\\\layers\\\\patch_embed.py\", line 136, in forward\\n    x = self.proj(x)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1776, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1787, in _call_impl\\n    return forward_call(*args, **kwargs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\conv.py\", line 553, in forward\\n    return self._conv_forward(input, self.weight, self.bias)\\n           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\conv.py\", line 548, in _conv_forward\\n    return F.conv2d(\\n           ~~~~~~~~^\\n        input, weight, bias, self.stride, self.padding, self.dilation, self.groups\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n'], 'current_inputs': {'focal_length_mm': [30], 'output_prefix': ['sharp'], 'model': [\"{'model_path': 'F:\\\\\\\\ComfyUI_windows_portable\\\\\\\\ComfyUI\\\\\\\\models\\\\\\\\sharp\\\\\\\\sharp_2572gikvuh.pt', 'dtype': torch.bfloat16}\"], 'image': ['tensor([[[[0.0039, 0.4863, 0.9608],\\n          [0.0039, 0.4863, 0.9608],\\n          [0.0039, 0.4863, 0.9608],\\n          ...,\\n          [0.0510, 0.0353, 0.0000],\\n          [0.0510, 0.0353, 0.0000],\\n          [0.0510, 0.0353, 0.0000]],\\n\\n         [[0.0039, 0.4863, 0.9608],\\n          [0.0039, 0.4863, 0.9608],\\n          [0.0039, 0.4863, 0.9608],\\n          ...,\\n          [0.0510, 0.0353, 0.0000],\\n          [0.0510, 0.0353, 0.0000],\\n          [0.0510, 0.0353, 0.0000]],\\n\\n         [[0.0039, 0.4863, 0.9608],\\n          [0.0039, 0.4863, 0.9608],\\n          [0.0039, 0.4863, 0.9608],\\n          ...,\\n          [0.0510, 0.0353, 0.0000],\\n          [0.0510, 0.0353, 0.0000],\\n          [0.0510, 0.0353, 0.0000]],\\n\\n         ...,\\n\\n         [[0.6824, 0.6157, 0.5373],\\n          [0.6824, 0.6157, 0.5373],\\n          [0.6824, 0.6157, 0.5373],\\n          ...,\\n          [0.5137, 0.4627, 0.4275],\\n          [0.5137, 0.4627, 0.4314],\\n          [0.5137, 0.4627, 0.4275]],\\n\\n         [[0.6824, 0.6157, 0.5373],\\n          [0.6824, 0.6157, 0.5373],\\n          [0.6824, 0.6157, 0.5373],\\n          ...,\\n          [0.5059, 0.4667, 0.4314],\\n          [0.5059, 0.4627, 0.4392],\\n          [0.5059, 0.4667, 0.4314]],\\n\\n         [[0.6824, 0.6157, 0.5373],\\n          [0.6824, 0.6157, 0.5373],\\n          [0.6824, 0.6157, 0.5373],\\n          ...,\\n          [0.5059, 0.4627, 0.4392],\\n          [0.5059, 0.4627, 0.4392],\\n          [0.5059, 0.4627, 0.4392]]]])']}, 'current_outputs': ['4', '2', '5', '1'], 'timestamp': 1771257128784}\n\nDetails:\nWorkflow: F:\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-Sharp\\workflows\\sharp_basic.json\nNode error: SharpPredict",
      "hardware": {
        "os": "Windows-11-10.0.26100-SP0",
        "cpu": "Intel64 Family 6 Model 58 Stepping 9, GenuineIntel",
        "gpu": "NVIDIA RTX A4000"
      },
      "resources": {
        "ram": {
          "peak": 25.36,
          "avg": 21.45
        },
        "total_ram_gb": 30.0,
        "samples": 40,
        "vram": {
          "peak": 3.24,
          "avg": 1.71
        }
      }
    },
    {
      "name": "sharp_exif",
      "status": "fail",
      "duration_seconds": 53.64,
      "error": "Workflow execution failed: {'prompt_id': 'ed21c745-69af-45c7-a545-e784297b6ac1', 'node_id': '7', 'node_type': 'SharpPredict', 'executed': ['9', '2'], 'exception_message': 'Input type (float) and bias type (struct c10::BFloat16) should be the same\\n', 'exception_type': 'RuntimeError', 'traceback': ['  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\execution.py\", line 527, in execute\\n    output_data, output_ui, has_subgraph, has_pending_tasks = await get_output_data(prompt_id, unique_id, obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb, v3_data=v3_data)\\n                                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\execution.py\", line 331, in get_output_data\\n    return_values = await _async_map_node_over_list(prompt_id, unique_id, obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb, v3_data=v3_data)\\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\execution.py\", line 305, in _async_map_node_over_list\\n    await process_inputs(input_dict, i)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\execution.py\", line 293, in process_inputs\\n    result = f(**inputs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\utils\\\\_contextlib.py\", line 124, in decorate_context\\n    return func(*args, **kwargs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\custom_nodes\\\\ComfyUI-Sharp\\\\nodes\\\\predict.py\", line 190, in predict\\n    gaussians = self._predict_image_cached(\\n        predictor, image_np, f_px, device,\\n        extrinsics=img_extrinsics,\\n        intrinsics=img_intrinsics,\\n    )\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\custom_nodes\\\\ComfyUI-Sharp\\\\nodes\\\\predict.py\", line 284, in _predict_image_cached\\n    monodepth_output, _ = predictor.encode(image_resized_pt)\\n                          ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\custom_nodes\\\\ComfyUI-Sharp\\\\nodes\\\\sharp\\\\models\\\\predictor.py\", line 112, in encode\\n    monodepth_output = self.monodepth_model(image)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1776, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1787, in _call_impl\\n    return forward_call(*args, **kwargs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\custom_nodes\\\\ComfyUI-Sharp\\\\nodes\\\\sharp\\\\models\\\\monodepth.py\", line 197, in forward\\n    encoder_output = self.monodepth_predictor.encoder(inputs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1776, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1787, in _call_impl\\n    return forward_call(*args, **kwargs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\custom_nodes\\\\ComfyUI-Sharp\\\\nodes\\\\sharp\\\\models\\\\encoders\\\\spn_encoder.py\", line 249, in forward\\n    x_pyramid_encodings, patch_intermediate_features = self.patch_encoder(x_pyramid_patches)\\n                                                       ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1776, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1787, in _call_impl\\n    return forward_call(*args, **kwargs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\ComfyUI\\\\custom_nodes\\\\ComfyUI-Sharp\\\\nodes\\\\sharp\\\\models\\\\encoders\\\\vit_encoder.py\", line 70, in forward\\n    x = self.patch_embed(input_tensor)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1776, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1787, in _call_impl\\n    return forward_call(*args, **kwargs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\timm\\\\layers\\\\patch_embed.py\", line 136, in forward\\n    x = self.proj(x)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1776, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py\", line 1787, in _call_impl\\n    return forward_call(*args, **kwargs)\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\conv.py\", line 553, in forward\\n    return self._conv_forward(input, self.weight, self.bias)\\n           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\conv.py\", line 548, in _conv_forward\\n    return F.conv2d(\\n           ~~~~~~~~^\\n        input, weight, bias, self.stride, self.padding, self.dilation, self.groups\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n    )\\n    ^\\n'], 'current_inputs': {'focal_length_mm': [20], 'output_prefix': ['sharp'], 'model': [\"{'model_path': 'F:\\\\\\\\ComfyUI_windows_portable\\\\\\\\ComfyUI\\\\\\\\models\\\\\\\\sharp\\\\\\\\sharp_2572gikvuh.pt', 'dtype': torch.bfloat16}\"], 'image': ['tensor([[[[0.2157, 0.1647, 0.1569],\\n          [0.2275, 0.1608, 0.1608],\\n          [0.2275, 0.1647, 0.1529],\\n          ...,\\n          [0.4471, 0.4745, 0.4863],\\n          [0.4510, 0.4745, 0.4863],\\n          [0.4431, 0.4745, 0.4745]],\\n\\n         [[0.2196, 0.1686, 0.1569],\\n          [0.2314, 0.1647, 0.1608],\\n          [0.2353, 0.1647, 0.1647],\\n          ...,\\n          [0.4549, 0.4784, 0.4706],\\n          [0.4549, 0.4706, 0.4784],\\n          [0.4431, 0.4706, 0.4667]],\\n\\n         [[0.2314, 0.1725, 0.1569],\\n          [0.2510, 0.1725, 0.1608],\\n          [0.2392, 0.1725, 0.1686],\\n          ...,\\n          [0.4588, 0.4745, 0.4627],\\n          [0.4510, 0.4745, 0.4667],\\n          [0.4471, 0.4706, 0.4627]],\\n\\n         ...,\\n\\n         [[0.1569, 0.1647, 0.1843],\\n          [0.1647, 0.1725, 0.1882],\\n          [0.1490, 0.1686, 0.1922],\\n          ...,\\n          [0.1020, 0.1137, 0.1373],\\n          [0.1137, 0.1176, 0.1412],\\n          [0.0863, 0.1176, 0.1373]],\\n\\n         [[0.1569, 0.1686, 0.1804],\\n          [0.1569, 0.1647, 0.1961],\\n          [0.1490, 0.1647, 0.1961],\\n          ...,\\n          [0.1059, 0.1216, 0.1294],\\n          [0.1176, 0.1216, 0.1333],\\n          [0.1059, 0.1137, 0.1373]],\\n\\n         [[0.1608, 0.1686, 0.1843],\\n          [0.1608, 0.1686, 0.1882],\\n          [0.1569, 0.1686, 0.1882],\\n          ...,\\n          [0.1059, 0.1176, 0.1333],\\n          [0.1137, 0.1176, 0.1333],\\n          [0.1333, 0.1176, 0.1333]]]])']}, 'current_outputs': ['9', '8', '2', '7'], 'timestamp': 1771257182827}\n\nDetails:\nWorkflow: F:\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-Sharp\\workflows\\sharp_exif.json\nNode error: SharpPredict",
      "hardware": {
        "os": "Windows-11-10.0.26100-SP0",
        "cpu": "Intel64 Family 6 Model 58 Stepping 9, GenuineIntel",
        "gpu": "NVIDIA RTX A4000"
      },
      "resources": {
        "ram": {
          "peak": 24.97,
          "avg": 20.52
        },
        "total_ram_gb": 30.0,
        "samples": 42,
        "vram": {
          "peak": 2.96,
          "avg": 1.46
        }
      }
    }
  ]
}